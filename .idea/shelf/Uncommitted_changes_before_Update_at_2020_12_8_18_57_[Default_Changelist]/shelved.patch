Index: TestCal500.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding=utf-8 -*-\r\nimport csv\r\nimport numpy as np\r\n\r\nfrom sklearn.utils import shuffle\r\n\r\nfrom learner.cascade import Cascade\r\nfrom learner.measure import *\r\n\r\n\r\n# 随机排列实例数，将实例划分为训练集和测试集\r\ndef shuffle_index(num_samples):\r\n    # a = range(0, 502),502是实例数\r\n    a = range(0, num_samples)\r\n\r\n    # 利用shuffle函数将序列a中的元素重新随机排列\r\n    a = shuffle(a)\r\n\r\n    # 去实例数的一半，上取整\r\n    length = int((num_samples + 1) / 2)\r\n    # 上半做训练集\r\n    train_index = a[:length]\r\n    # 下半做测试集\r\n    test_index = a[length:]\r\n    return [train_index, test_index]\r\n\r\n\r\n# 加载数据和标签\r\ndef load_csv():\r\n    \"\"\"\r\n    从CSV文件中读取数据信息\r\n    :param csv_file_name: CSV文件名\r\n    :return: Data：二维数组\r\n    \"\"\"\r\n    # CAL500数据集是music数据集\r\n    # shape是(502, 242)，即502个实例；242 = features + labels\r\n    # 前68列是features（个人认为应该是一个音频的特征表示，类比于一张图片的像素点表示）；\r\n    # 后174列是labels，一个music实例有174个标签；\r\n    data_csv = r'D:\\Pycharm2020.1.3\\WorkSpace\\MLDF\\dataset\\CAL500_data.csv'\r\n    label_csv = r'D:\\Pycharm2020.1.3\\WorkSpace\\MLDF\\dataset\\CAL500_label.csv'\r\n    with open(data_csv, encoding='utf-8') as f:\r\n        data = np.loadtxt(f, str, delimiter=\",\")\r\n    with open(label_csv, encoding='utf-8') as f:\r\n        label = np.loadtxt(f, str, delimiter=\",\")\r\n    # 将数据label强制转换为指定的类型，astype函数是在副本上进行，并非修改原数组。\r\n    # 从文件中load出来的数据类型是“class 'numpy.int16'”类型，需要进行类型转化\r\n    label = label.astype(\"int\")\r\n    # 取数据集的行数，即是实例数\r\n    num_samples = len(data)\r\n\r\n    # 用shuffle_index函数将502这个整数随机划分成两个长为251的list，list中的元素是502以内的整数\r\n    # data是<class 'numpy.ndarray'>的二维矩阵，将上一步的list传入，会将data中按list中的元素按行取出\r\n    # 这两步就是将（502，68）的data二维矩阵划分成了两个（251，68）的二维矩阵，分别代表训练集和测试集\r\n    # 针对label这个（502, 174）的二维矩阵也是这么操作，而且采集时用的是同一组list，保证实例和标签对应\r\n    train_index, test_index = shuffle_index(num_samples)\r\n    \"\"\"\r\n    划分结果如下：\r\n    train_data <class 'numpy.ndarray'> (251, 68) \r\n    train_label <class 'numpy.ndarray'> (251, 174) \r\n    test_data <class 'numpy.ndarray'> (251, 68) \r\n    test_label <class 'numpy.ndarray'> (251, 174) \r\n    \"\"\"\r\n    train_data = data[train_index]\r\n    train_label = label[train_index]\r\n    test_data = data[test_index]\r\n    test_label = label[test_index]\r\n\r\n\r\n    print(\"加载CAL500数据集完成！！!\")\r\n\r\n    # 返回值是训练数据、测试数据、标签数\r\n    return [train_data, train_label, test_data, test_label]\r\n\r\n\r\nif __name__ == '__main__':\r\n    dataset = \"CAL500\"\r\n    # 初始化数据集、测试数据集、标签集\r\n    train_data, train_label, test_data, test_label = load_csv()\r\n\r\n    # 构造森林，将另个森林级联，最大层数设为10，5折交叉验证\r\n    model = Cascade(dataset, max_layer=20, num_forests=4, n_fold=5, step=3)\r\n\r\n    # 训练森林，传入训练集、训练标签、指标名称、每个森林中的树的数量设为40\r\n    model.train(train_data, train_label, \"hamming loss\", n_estimators=40)\r\n\r\n    test_prob = model.predict(test_data, \"hamming loss\")\r\n\r\n    value = do_metric(test_prob, test_label, 0.5)\r\n\r\n    meatures = [\"hamming loss\", \"one-error\", \"coverage\", \"ranking loss\", \"average precision\", \"macro-auc\"]\r\n    res = zip(meatures, value)\r\n    for item in res:\r\n        print(item)\r\n\"\"\"\r\nmax_layer=20, num_forests=4, n_fold=5, step=3\r\n('hamming loss', 0.13836607592618033)\r\n('one-error', 0.13545816733067728)\r\n('coverage', 0.7360214315153181)\r\n('ranking loss', 0.17821650587017102)\r\n('average precision', 0.5018073210966437)\r\n('macro-auc', 0.5650149637792449)\r\n\r\nmax_layer=30, num_forests=3, n_fold=5, step=3\r\n('hamming loss', 0.14894445207675044)\r\n('one-error', 0.9203187250996016)\r\n('coverage', 0.9863763337454778)\r\n('ranking loss', 0.9984451995761596)\r\n('average precision', 0.14508951038068932)\r\n('macro-auc', 0.5001274023615888)\r\n\"\"\"\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- TestCal500.py	(revision 3644eedc425028d40d752162e54195458b8c4596)
+++ TestCal500.py	(date 1607424987804)
@@ -1,7 +1,4 @@
 # -*- coding=utf-8 -*-
-import csv
-import numpy as np
-
 from sklearn.utils import shuffle
 
 from learner.cascade import Cascade
