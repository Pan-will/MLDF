Index: Iris/Iris.py
===================================================================
--- Iris/Iris.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ Iris/Iris.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
@@ -1,60 +0,0 @@
-# 引入数据集，sklearn包含众多数据集
-from sklearn import datasets
-# 将数据分为测试集和训练集
-from sklearn.model_selection import train_test_split
-# 利用邻近点方式训练数据
-from sklearn.neighbors import KNeighborsClassifier
-
-# 引入数据,本次导入鸢尾花数据，iris数据包含4个特征变量
-iris = datasets.load_iris()
-# 特征变量
-iris_X = iris.data
-print('特征矩阵的规模和长度', iris_X.shape, len(iris_X))
-# 目标值
-iris_y = iris.target
-print('鸢尾花的目标值', iris_y)
-# 利用train_test_split进行训练集和测试机进行分开，test_size占30%
-X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3)
-# 我们看到训练数据的特征值分为3类
-# print(y_train)
-'''
-[1 1 0 2 0 0 0 2 2 2 1 0 2 0 2 1 0 1 0 2 0 1 0 0 2 1 2 0 0 1 0 0 1 0 0 0 0
- 2 2 2 1 1 1 2 0 2 0 1 1 1 1 2 2 1 2 2 2 0 2 2 2 0 1 0 1 0 0 1 2 2 2 1 1 1
- 2 0 0 1 0 2 1 2 0 1 2 2 2 1 2 1 0 0 1 0 0 1 1 1 0 2 1 1 0 2 2]
-'''
-# 训练数据
-# 引入训练方法
-knn = KNeighborsClassifier()
-# 进行填充测试数据进行训练
-knn.fit(X_train, y_train)
-
-params = knn.get_params()
-# print(params)
-'''
-{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski',
- 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5,
- 'p': 2, 'weights': 'uniform'}
-'''
-
-score = knn.score(X_test, y_test)
-print("预测得分为：%s" % score)
-'''
-预测得分为：0.9555555555555556
-[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 2 2 0 2 2 2 2 1 2 2 2 1 2 2 1 2 0
- 2 1 2 1 1 0 2 1]
-[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 1 2 0 2 2 2 2 1 1 2 2 1 2 2 1 2 0
- 2 1 2 1 1 0 2 1]
-'''
-
-# 预测数据，预测特征值
-print("预测特征值:", knn.predict(X_test))
-'''
-[1 1 0 1 1 2 0 0 1 0 1 0 2 1 0 1 2 0 2 0 2 2 1 1 2 0 1 1 0 1 0 1 1 1 0 1 0
- 2 1 2 0 0 0 1 1]
-'''
-# 打印真实特征值
-print("打印真实特征值:", y_test)
-'''
-[1 1 0 1 1 2 0 0 1 0 1 0 2 2 0 1 2 0 2 0 2 2 1 1 2 0 1 1 0 1 0 1 1 1 0 1 0
- 2 1 2 0 0 0 1 1]
-'''
Index: Iris/Iris2.py
===================================================================
--- Iris/Iris2.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ Iris/Iris2.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
@@ -1,149 +0,0 @@
-# utf-8
-
-from sklearn import datasets
-# from sklearn.cross_validation import train_test_split, cross_val_score
-# 将数据分为测试集和训练集
-from sklearn.model_selection import train_test_split, cross_val_score
-# from sklearn.learning_curve import validation_curve
-from sklearn.model_selection import validation_curve
-
-from sklearn.neighbors import KNeighborsClassifier
-from sklearn.linear_model import LinearRegression
-from sklearn.svm import SVC
-from sklearn import preprocessing
-import matplotlib.pyplot as plt
-import numpy as np
-
-
-def knn_test():
-    '''
-    knn实现iris的分类
-    '''
-    iris = datasets.load_iris()
-    iris_x = iris.data
-    iris_y = iris.target
-    # print(iris_x[:10])
-    # print(iris_y[:10])
-    x_train, x_test, y_train, y_test = train_test_split(iris_x[:100], iris_y[:100], test_size=0.3)
-    print(y_train)
-
-    knn = KNeighborsClassifier()
-    knn.fit(x_train, y_train)
-    print(knn.predict(x_test))
-    print(y_test)
-
-
-def knn_test2():
-    '''
-    knn实现iris的分类,并且使用交叉验证，并且划分成多次进行交叉验证，得到一个准确度列表scores
-    '''
-    iris = datasets.load_iris()
-    iris_x = iris.data
-    iris_y = iris.target
-    x_train, x_test, y_train, y_test = train_test_split(iris_x[:100], iris_y[:100], test_size=0.3)
-    knn = KNeighborsClassifier(n_neighbors=5)
-    scores = cross_val_score(knn, iris_x, iris_y, cv=5, scoring='accuracy')  # cross_val_score for classfication
-    print(scores)
-
-
-def linear_test():
-    '''
-    生成数据，使用linear regression实现回归，画图
-    '''
-    # boston = datasets.load_boston()
-    # x = boston.data
-    # y = boston.target
-    x, y = datasets.make_regression(n_samples=50, n_features=1, noise=1)
-    x_train, x_test, y_train, y_test = train_test_split(
-        x[:100], y[:100], test_size=0.3)
-    linear = LinearRegression()
-    linear.fit(x_train, y_train)
-    linear.predict(x[:4])
-    print(linear.score(x_test, y_test))
-    plt.scatter(x, y)
-    plt.show()
-
-
-def normalization():
-    '''
-    归一化（正则化）,使用svm进行分类，并画图比较正则化前后的准确率
-    '''
-    x, y = datasets.make_classification(n_samples=300, n_features=2, n_redundant=0,
-                                        n_informative=1, random_state=22, n_clusters_per_class=1, scale=100)
-    x_train, x_test, y_train, y_test = train_test_split(
-        x[:100], y[:100], test_size=0.3)
-    model = SVC()
-    model.fit(x_train, y_train)
-    score1 = model.score(x_test, y_test)
-    # print(x[:5], y[:5])
-    plt.subplot(121)
-    plt.scatter(x[:, 0], x[:, 1], c=y)
-
-    x = preprocessing.scale(x)
-    y = preprocessing.scale(y)
-    x_train, x_test, y_train, y_test = train_test_split(
-        x[:100], y[:100], test_size=0.3)
-    model = SVC()
-    model.fit(x_train, y_train)
-    score2 = model.score(x_test, y_test)
-    # print(x[:5], y[:5])
-    plt.subplot(122)
-    plt.scatter(x[:, 0], x[:, 1], c=y)
-    print('precision:', score1)
-    print('precision:', score2)
-    plt.show()
-
-
-def param_select():
-    '''
-    选择合适的knn参数k，分别在分类、回归
-    '''
-    iris = datasets.load_iris()
-    x = iris.data
-    y = iris.target
-    k_range = range(1, 30)
-    k_scores = []
-    for k in k_range:
-        knn = KNeighborsClassifier(n_neighbors=k)
-        # loss=-cross_val_score(knn,x,y,cv=10,scoring="mean_squared_error")# for regression
-        # k_scores.append(loss.mean())
-        scores = cross_val_score(
-            knn, x, y, cv=10, scoring='accuracy')  # for classification
-        k_scores.append(scores.mean())
-    plt.plot(k_range, k_scores)
-    plt.xlabel('value of k for knn')
-    # plt.ylabel('crowss validated loss')
-    plt.ylabel('crowss validated accuracy')
-    plt.show()
-
-
-def validation_curve_test():
-    '''
-    使用validation curve观察学习曲线，此处展示了过拟合的情况
-    '''
-    digits = datasets.load_digits()
-    x = digits.data
-    y = digits.target
-    model = SVC()
-    param_range = np.logspace(-6, -2.3, 5)
-    train_loss, test_loss = validation_curve(model, x, y, param_name='gamma', param_range=param_range, cv=10,
-                                             scoring='mean_squared_error')  # 数据大小，训练曲线、测试曲线
-
-    train_loss_mean = -np.mean(train_loss, axis=1)
-    test_loss_mean = -np.mean(test_loss, axis=1)
-
-    plt.plot(param_range, train_loss_mean, label='train')
-    plt.plot(param_range, test_loss_mean, label='cross-validation')
-    plt.xlabel('gamma')
-    plt.ylabel('Loss')
-    plt.legend(loc='best')
-    plt.show()
-
-
-if __name__ == '__main__':
-    knn_test()
-    linear_test()
-    normalization()
-    knn_test2()
-    param_select()
-    validation_curve_test()
\ No newline at end of file
Index: Iris/Hastie.py
===================================================================
--- Iris/Hastie.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ Iris/Hastie.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
@@ -1,49 +0,0 @@
-import matplotlib.pyplot as plt
-
-from sklearn.datasets import make_classification
-from sklearn.datasets import make_blobs
-from sklearn.datasets import make_gaussian_quantiles
-from sklearn.datasets import make_hastie_10_2
-
-plt.figure(figsize=(8, 8))
-plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)
-
-plt.subplot(421)
-plt.title("One informative feature, one cluster per class", fontsize='small')
-X1, Y1 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1,
-                             n_clusters_per_class=1)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-
-plt.subplot(422)
-plt.title("Two informative features, one cluster per class", fontsize='small')
-X1, Y1 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
-                             n_clusters_per_class=1)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-
-plt.subplot(423)
-plt.title("Two informative features, two clusters per class", fontsize='small')
-X2, Y2 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2)
-plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)
-
-plt.subplot(424)
-plt.title("Multi-class, two informative features, one cluster",
-          fontsize='small')
-X1, Y1 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
-                             n_clusters_per_class=1, n_classes=3)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-
-plt.subplot(425)
-plt.title("Three blobs", fontsize='small')
-X1, Y1 = make_blobs(n_samples=1000, n_features=2, centers=3)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-
-plt.subplot(426)
-plt.title("Gaussian divided into four quantiles", fontsize='small')
-X1, Y1 = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=4)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-
-plt.subplot(427)
-plt.title("hastie data ", fontsize='small')
-X1, Y1 = make_hastie_10_2(n_samples=1000)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
-plt.show()
\ No newline at end of file
Index: Iris/test.py
===================================================================
--- Iris/test.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ Iris/test.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
@@ -1,2 +0,0 @@
-print(0 & 0, 0 or 0, 0 | 0, 0 and 0)
-print(0 & 1, 1 & 1)
Index: TestCal500.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding=utf-8 -*-\r\nimport csv\r\nimport numpy as np\r\n\r\nfrom sklearn.utils import shuffle\r\n\r\nfrom learner.cascade import Cascade\r\nfrom learner.measure import *\r\n\r\n\r\n# 随机排列实例数，将实例划分为训练集和测试集\r\ndef shuffle_index(num_samples):\r\n    # a = range(0, 502),502是实例数\r\n    a = range(0, num_samples)\r\n\r\n    # 利用shuffle函数将序列a中的元素重新随机排列\r\n    a = shuffle(a)\r\n\r\n    # 去实例数的一半，上取整\r\n    length = int((num_samples + 1) / 2)\r\n    # 上半做训练集\r\n    train_index = a[:length]\r\n    # 下半做测试集\r\n    test_index = a[length:]\r\n    return [train_index, test_index]\r\n\r\n\r\n# 加载数据和标签\r\ndef load_csv():\r\n    \"\"\"\r\n    从CSV文件中读取数据信息\r\n    :param csv_file_name: CSV文件名\r\n    :return: Data：二维数组\r\n    \"\"\"\r\n    # CAL500数据集是music数据集\r\n    # shape是(502, 242)，即502个实例；242 = features + labels\r\n    # 前68列是features（个人认为应该是一个音频的特征表示，类比于一张图片的像素点表示）；\r\n    # 后174列是labels，一个music实例有174个标签；\r\n    data_csv = r'D:\\Pycharm2020.1.3\\WorkSpace\\MLDF\\dataset\\CAL500_data.csv'\r\n    label_csv = r'D:\\Pycharm2020.1.3\\WorkSpace\\MLDF\\dataset\\CAL500_label.csv'\r\n    with open(data_csv, encoding='utf-8') as f:\r\n        data = np.loadtxt(f, str, delimiter=\",\")\r\n    with open(label_csv, encoding='utf-8') as f:\r\n        label = np.loadtxt(f, str, delimiter=\",\")\r\n    # 将数据label强制转换为指定的类型，astype函数是在副本上进行，并非修改原数组。\r\n    # 从文件中load出来的数据类型是“class 'numpy.int16'”类型，需要进行类型转化\r\n    label = label.astype(\"int\")\r\n    # 取数据集的行数，即是实例数\r\n    num_samples = len(data)\r\n\r\n    # 用shuffle_index函数将502这个整数随机划分成两个长为251的list，list中的元素是502以内的整数\r\n    # data是<class 'numpy.ndarray'>的二维矩阵，将上一步的list传入，会将data中按list中的元素按行取出\r\n    # 这两步就是将（502，68）的data二维矩阵划分成了两个（251，68）的二维矩阵，分别代表训练集和测试集\r\n    # 针对label这个（502, 174）的二维矩阵也是这么操作，而且采集时用的是同一组list，保证实例和标签对应\r\n    train_index, test_index = shuffle_index(num_samples)\r\n    \"\"\"\r\n    划分结果如下：\r\n    train_data <class 'numpy.ndarray'> (251, 68) \r\n    train_label <class 'numpy.ndarray'> (251, 174) \r\n    test_data <class 'numpy.ndarray'> (251, 68) \r\n    test_label <class 'numpy.ndarray'> (251, 174) \r\n    \"\"\"\r\n    train_data = data[train_index]\r\n    train_label = label[train_index]\r\n    test_data = data[test_index]\r\n    test_label = label[test_index]\r\n\r\n\r\n    print(\"加载CAL500数据集完成！！!\")\r\n\r\n    # 返回值是训练数据、测试数据、标签数\r\n    return [train_data, train_label, test_data, test_label]\r\n\r\n\r\nif __name__ == '__main__':\r\n    dataset = \"CAL500\"\r\n    # 初始化数据集、测试数据集、标签集\r\n    train_data, train_label, test_data, test_label = load_csv()\r\n\r\n    # 构造森林，将另个森林级联，最大层数设为10，5折交叉验证\r\n    model = Cascade(dataset, max_layer=20, num_forests=4, n_fold=5, step=3)\r\n\r\n    # 训练森林，传入训练集、训练标签、指标名称、每个森林中的树的数量设为40\r\n    model.train(train_data, train_label, \"hamming loss\", n_estimators=40)\r\n\r\n    test_prob = model.predict(test_data, \"hamming loss\")\r\n\r\n    value = do_metric(test_prob, test_label, 0.5)\r\n\r\n    meatures = [\"hamming loss\", \"one-error\", \"coverage\", \"ranking loss\", \"average precision\", \"macro-auc\"]\r\n    res = zip(meatures, value)\r\n    for item in res:\r\n        print(item)\r\n\"\"\"\r\nmax_layer=20, num_forests=4, n_fold=5, step=3\r\n('hamming loss', 0.13836607592618033)\r\n('one-error', 0.13545816733067728)\r\n('coverage', 0.7360214315153181)\r\n('ranking loss', 0.17821650587017102)\r\n('average precision', 0.5018073210966437)\r\n('macro-auc', 0.5650149637792449)\r\n\r\nmax_layer=30, num_forests=3, n_fold=5, step=3\r\n('hamming loss', 0.14894445207675044)\r\n('one-error', 0.9203187250996016)\r\n('coverage', 0.9863763337454778)\r\n('ranking loss', 0.9984451995761596)\r\n('average precision', 0.14508951038068932)\r\n('macro-auc', 0.5001274023615888)\r\n\"\"\"\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- TestCal500.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ TestCal500.py	(date 1607325708987)
@@ -77,7 +77,7 @@
     # 初始化数据集、测试数据集、标签集
     train_data, train_label, test_data, test_label = load_csv()
 
-    # 构造森林，将另个森林级联，最大层数设为10，5折交叉验证
+    # 构造级联结构，将4个森林级联，最大层数设为20，5折交叉验证，我们将第一层的最大深度设置为3，然后在增加层数时比上一层多3倍
     model = Cascade(dataset, max_layer=20, num_forests=4, n_fold=5, step=3)
 
     # 训练森林，传入训练集、训练标签、指标名称、每个森林中的树的数量设为40
Index: TestImage.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from sklearn.utils import shuffle\r\n\r\nfrom learner.cascade import Cascade\r\nfrom learner.measure import *\r\n\r\n\r\n# 随机排列实例数，将实例划分为训练集和测试集\r\ndef shuffle_index(num_samples):\r\n    a = range(0, num_samples)\r\n    # 利用shuffle函数将序列a中的元素重新随机排列\r\n    a = shuffle(a)\r\n    # 去实例数的一半，上取整\r\n    length = int((num_samples + 1) / 2)\r\n    # 上半做训练集\r\n    train_index = a[:length]\r\n    # 下半做测试集\r\n    test_index = a[length:]\r\n    return [train_index, test_index]\r\n\r\n\r\n# 加载数据和标签\r\n# image数据集一共2000条数据，每条数据5个标签\r\n# data矩阵shape是2000，294；label矩阵shape是2000，5\r\ndef make_data(dataset):\r\n    data = np.load(\"dataset/{}_data.npy\".format(dataset))\r\n    label = np.load(\"dataset/{}_label.npy\".format(dataset))\r\n\r\n    print(\"data矩阵信息：\", data.shape, type(data[0]), data[0].shape)\r\n    # 将数据label强制转换为指定的类型，astype函数是在副本上进行，并非修改原数组。\r\n    # 从文件中load出来的数据类型是“class 'numpy.int16'”类型，需要进行类型转化\r\n    label = label.astype(\"int\")\r\n    print(\"label矩阵信息：\", label.shape, type(label[0]), label[0].shape)\r\n    # 取数据集的行数，即是实例数：2000个实例\r\n    num_samples = data.shape[0]\r\n\r\n    # 用shuffle_index函数将2000这个整数随机划分成两个长为1000的list，list中的元素是2000以内的整数\r\n    # data是<class 'numpy.ndarray'>的二维矩阵，将上一步的list传入，会将data中按list中的元素按行取出\r\n    # 这两步就是将（2000，294）的data二维矩阵划分成了两个（1000，294）的二维矩阵，分别代表训练集和测试集\r\n    # 针对label这个（2000，5）的二维矩阵也是这么操作，而且采集时用的是同一组list，保证实例和标签对应\r\n    train_index, test_index = shuffle_index(num_samples)\r\n    train_data = data[train_index]\r\n    train_label = label[train_index]\r\n    test_data = data[test_index]\r\n    test_label = label[test_index]\r\n    # print(\"train_data\", train_data)\r\n    # print(\"train_label\", train_label)\r\n    # print(\"test_data\", test_data)\r\n    # print(\"test_label\", test_label)\r\n    return [train_data, train_label, test_data, test_label]\r\n\r\n\"\"\"\r\n数据集文件：data、label文件\r\n\"\"\"\r\nif __name__ == '__main__':\r\n    dataset = \"image\"\r\n    # 初始化数据集、标签集、测试数据标签集\r\n    train_data, train_label, test_data, test_label = make_data(dataset)\r\n\r\n    # 构造森林，将另个森林级联，最大层数设为10，5重交叉验证\r\n    model = Cascade(dataset, max_layer=10, num_forests=2, n_fold=5, step=3)\r\n    # 训练森林，传入训练集、训练标签、指标名称、每个森林中的树的数量设为40\r\n    model.train(train_data, train_label, \"hamming loss\", n_estimators=40)\r\n\r\n    test_prob = model.predict(test_data, \"hamming loss\")\r\n\r\n    value = do_metric(test_prob, test_label, 0.5)\r\n    meatures = [\"hamming loss\", \"one-error\", \"coverage\", \"ranking loss\", \"average precision\", \"macro-auc\"]\r\n    res = zip(meatures, value)\r\n    for item in res:\r\n        print(item)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- TestImage.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ TestImage.py	(date 1607343437860)
@@ -58,13 +58,16 @@
 
     # 构造森林，将另个森林级联，最大层数设为10，5重交叉验证
     model = Cascade(dataset, max_layer=10, num_forests=2, n_fold=5, step=3)
+
     # 训练森林，传入训练集、训练标签、指标名称、每个森林中的树的数量设为40
-    model.train(train_data, train_label, "hamming loss", n_estimators=40)
+    model.train(train_data, train_label, "average precision", n_estimators=40)
 
-    test_prob = model.predict(test_data, "hamming loss")
+    test_prob = model.predict(test_data, "average precision")
 
+    # 计算各多标签评估指标的值。参数列表：测试数据的预测概率，测试数据的真实标签，临界值（预测概率大于该临界值才可取）
+    # 此类不用改，直接拿来用。
     value = do_metric(test_prob, test_label, 0.5)
-    meatures = ["hamming loss", "one-error", "coverage", "ranking loss", "average precision", "macro-auc"]
+    meatures = ["hamming loss", "one-error", "coverage", "ranking loss", "average precision", "F1", "macro-auc"]
     res = zip(meatures, value)
     for item in res:
         print(item)
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># MLDF\r\n论文出处：出处：Yang L , Wu X Z , Jiang Y , et al. Multi-Label Learning with Deep Forest[J]. 2019.\r\n\r\n\r\n本篇论文主要学习算法的第一部分：特征重用机制。\r\n当前层求置信度，与阈值比较，小于某一标准，则重用上一层的表示。\r\n\r\n\r\n作者邮箱：yangl@lamda.nju.edu.cn\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- README.md	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ README.md	(date 1606956965217)
@@ -5,5 +5,9 @@
 本篇论文主要学习算法的第一部分：特征重用机制。
 当前层求置信度，与阈值比较，小于某一标准，则重用上一层的表示。
 
+MLDF是批处理场景。
+流场景改变时SDF，已经见刊。
+
+想把MIDF改成流场景。——结合SDF的文章
 
 作者邮箱：yangl@lamda.nju.edu.cn
Index: learner/warpper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\n\r\nfrom .Layer import Layer\r\n\r\n\r\nclass KfoldWarpper:\r\n    # 参数：森林数=2，每个森里中的树的数量=40，交叉验证的倍数=5，层序号（1~20，for循环ing），步数=3\r\n    def __init__(self, num_forests, n_estimators, n_fold, kf, layer_index, step=3):\r\n        self.num_forests = num_forests\r\n        self.n_estimators = n_estimators\r\n        self.n_fold = n_fold\r\n        self.kf = kf\r\n        self.layer_index = layer_index\r\n        self.step = step\r\n        # 最终模型集C = {layer 1，...,layer L}\r\n        self.model = []\r\n\r\n    def train(self, train_data, train_label):\r\n        \"\"\"\r\n        :param train_data:训练数据\r\n        :param train_label:对应标签\r\n        :return:\r\n            prob: array, whose shape is (num_samples, num_labels)，一个数组，实例数是行数，标签数是列数\r\n            prob_concatenate\r\n        \"\"\"\r\n        # 标签数是训练标签集的列数\r\n        self.num_labels = train_label.shape[1]\r\n\r\n        # 实例数、特征数分别是训练集的行数和列数，注意区分特征和标签\r\n        num_samples, num_features = train_data.shape\r\n\r\n        # 构造一个二维矩阵，规模是（实例数，标签数），构造的矩阵不为空\r\n        prob = np.empty([num_samples, self.num_labels])\r\n        # 构造一个二维矩阵，规模是（森林数，实例数，标签数）=（2，实例数，标签数），构造的矩阵不为空，用于放置预测结果\r\n        prob_concatenate = np.empty([self.num_forests, num_samples, self.num_labels])\r\n\r\n        fold = 0\r\n        # train_data维度：（1000, 304）\r\n        for train_index, test_index in self.kf:  # 进行k折交叉验证，在train_data里创建交叉验证的补充\r\n            # train_data的shape：(1204, 294)，切片有三个参数，第一个是块下标，后面两个跟二维数组一样\r\n            # 也就是每趟循环取一个训练数据、测试数据、训练数据对应标签\r\n            \"\"\"\r\n            原始的数据集划成了4份：train_data（251，68）、test_data（251，68）、train_label（251，174）、test_label（251，174）；\r\n            这里又把data部分划成了(167, 68)、(84, 68)；label部分划成了(167, 174)、(84, 174)\r\n            X_train: <class 'numpy.ndarray'> (167, 68) 167\r\n            X_val <class 'numpy.ndarray'> (84, 68) 84\r\n            y_train <class 'numpy.ndarray'> (167, 174) 167\r\n            \"\"\"\r\n            X_train = train_data[train_index, :]# 选出训练集\r\n            X_val = train_data[test_index, :]# 验证集\r\n            y_train = train_label[train_index, :]# 训练标签\r\n            # weight_train = weight[train_index]  # 训练集对应的权重\r\n\r\n            # 加入层：构建第fold个层类\r\n            # 构建层类，参数列表：每个森林树的数量=40，森林数量=2，标签数=5（不同数据集，标签数不同），步数=3，层序号，交叉验证倍数\r\n            layer = Layer(self.n_estimators, self.num_forests, self.num_labels, self.step, self.layer_index, fold)\r\n\r\n            # layer层的训练，参数：训练集，对应标签\r\n            layer.train(X_train, y_train)\r\n\r\n            self.model.append(layer)\r\n            fold += 1\r\n            # 做预测，参数是新划分的test矩阵，shape是（84，68），返回值是[预测值针对森林数取得均值， 按分类器存放的预测值]\r\n            prob[test_index], prob_concatenate[:, test_index, :] = layer.predict(X_val)\r\n        return [prob, prob_concatenate]\r\n\r\n    def predict(self, test_data):\r\n        test_prob = np.zeros([test_data.shape[0], self.num_labels])\r\n        test_prob_concatenate = np.zeros([self.num_forests, test_data.shape[0], self.num_labels])\r\n        for layer in self.model:\r\n            temp_prob, temp_prob_concatenate = layer.predict(test_data)\r\n            test_prob += temp_prob\r\n            test_prob_concatenate += temp_prob_concatenate\r\n        test_prob /= self.n_fold\r\n        test_prob_concatenate /= self.n_fold\r\n        return [test_prob, test_prob_concatenate]\r\n\r\n    def train_and_predict(self, train_data, train_label, test_data):\r\n        prob, prob_concatenate = self.train(train_data, train_label)\r\n        test_prob, test_prob_concatenate = self.predict(test_data)\r\n        return [prob, prob_concatenate, test_prob, test_prob_concatenate]\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- learner/warpper.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ learner/warpper.py	(date 1607350244176)
@@ -3,8 +3,9 @@
 from .Layer import Layer
 
 
+# K折交叉验证类，为了防止模型过拟合
 class KfoldWarpper:
-    # 参数：森林数=2，每个森里中的树的数量=40，交叉验证的倍数=5，层序号（1~20，for循环ing），步数=3
+    # 参数：森林数=2，每个森里中的树的数量=40，交叉验证的倍数=5，层序号（1~20，for循环ing），我们将第一层的最大深度设置为3，然后在增加层数时比上一层多3倍
     def __init__(self, num_forests, n_estimators, n_fold, kf, layer_index, step=3):
         self.num_forests = num_forests
         self.n_estimators = n_estimators
@@ -36,6 +37,7 @@
 
         fold = 0
         # train_data维度：（1000, 304）
+        # 遍历k折交叉验证返回的训练集和测试集索引
         for train_index, test_index in self.kf:  # 进行k折交叉验证，在train_data里创建交叉验证的补充
             # train_data的shape：(1204, 294)，切片有三个参数，第一个是块下标，后面两个跟二维数组一样
             # 也就是每趟循环取一个训练数据、测试数据、训练数据对应标签
@@ -46,13 +48,13 @@
             X_val <class 'numpy.ndarray'> (84, 68) 84
             y_train <class 'numpy.ndarray'> (167, 174) 167
             """
-            X_train = train_data[train_index, :]# 选出训练集
-            X_val = train_data[test_index, :]# 验证集
-            y_train = train_label[train_index, :]# 训练标签
+            X_train = train_data[train_index, :]  # 选出训练集
+            X_val = train_data[test_index, :]  # 验证集
+            y_train = train_label[train_index, :]  # 训练标签
             # weight_train = weight[train_index]  # 训练集对应的权重
 
             # 加入层：构建第fold个层类
-            # 构建层类，参数列表：每个森林树的数量=40，森林数量=2，标签数=5（不同数据集，标签数不同），步数=3，层序号，交叉验证倍数
+            # 构建层类，参数列表：每个森林树的数量=40，森林数量=2，标签数=5（不同数据集，标签数不同），步数=3，层序号
             layer = Layer(self.n_estimators, self.num_forests, self.num_labels, self.step, self.layer_index, fold)
 
             # layer层的训练，参数：训练集，对应标签
Index: learner/measure.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\n六个度量指标\r\n\"\"\"\r\nimport numpy as np\r\nfrom sklearn import metrics\r\n\r\n\r\n# 初始化指标初值\r\ndef init_supervise(supervise):\r\n    # 排名误差\r\n    if supervise == \"ranking loss\":\r\n        ranking_loss = 1.0\r\n        return ranking_loss\r\n    # 汉明损失\r\n    elif supervise == \"hamming loss\":\r\n        hamming_loss = 1.0\r\n        return hamming_loss\r\n    elif supervise == \"one-error\":\r\n        one_error = 1.0\r\n        return one_error\r\n    elif supervise == \"average precision\":\r\n        average_precision = 0.0\r\n        return average_precision\r\n    elif supervise == \"micro-f1\":\r\n        micro_f1 = 0.0\r\n        return micro_f1\r\n    elif supervise == \"macro-f1\":\r\n        macro_f1 = 0.0\r\n        return macro_f1\r\n    elif supervise == \"coverage\":\r\n        coverage = 1000.0\r\n        return coverage\r\n    elif supervise == \"macro_auc\":\r\n        macro_auc = 0.0\r\n        return macro_auc\r\n\r\n\r\n# 比较指标，参数是指标名称、先后两个指标值\r\ndef compare_supervise_value(supervise, supervise_value1, supervise_value2):\r\n    if supervise == \"ranking loss\" or supervise == \"hamming loss\" or supervise == \"one-error\" or supervise == \"coverage\":\r\n        if supervise_value1 > supervise_value2 + 1e-4:\r\n            return False\r\n        else:\r\n            return True\r\n    elif supervise == \"average precision\" or supervise == \"micro-f1\" or supervise == \"macro-f1\" or supervise == \"macro_auc\":\r\n        if supervise_value1 + 1e-4 < supervise_value2:\r\n            return False\r\n        else:\r\n            return True\r\n\r\n\r\n# 比较指标，参数是指标名称，y_prob,标签,阈值\r\ndef compute_supervise(supervise, y_prob, label, threshold):\r\n    predict = y_prob > threshold\r\n    if supervise == \"ranking loss\":\r\n        temp_ranking_loss = compute_ranking_loss(\r\n            y_prob, label)  # prob / y_prob\r\n        value = temp_ranking_loss\r\n    elif supervise == \"hamming loss\":\r\n        temp_hamming_loss = compute_hamming_loss(predict, label)\r\n        value = temp_hamming_loss\r\n    elif supervise == \"one-error\":\r\n        temp_one_error = compute_one_error(y_prob, label)\r\n        value = temp_one_error\r\n    elif supervise == \"average precision\":\r\n        temp_average_precision = compute_average_precision(y_prob, label)\r\n        value = temp_average_precision\r\n    elif supervise == \"micro-f1\":\r\n        temp_micro_f1 = compute_micro_f1(predict, label)\r\n        value = temp_micro_f1\r\n    elif supervise == \"macro-f1\":\r\n        temp_macro_f1 = compute_macro_f1(predict, label)\r\n        value = temp_macro_f1\r\n    elif supervise == \"coverage\":\r\n        temp_coverage = compute_coverage(y_prob, label)\r\n        value = temp_coverage\r\n    elif supervise == \"macro_auc\":\r\n        macro_auc = compute_auc(y_prob, label)\r\n        value = macro_auc\r\n    return value\r\n\r\n\r\n# 计算指标向量\r\ndef compute_supervise_vec(supervise, y_prob, label, threshold):\r\n    # predict矩阵信息： <class 'numpy.ndarray'> (251, 174) 251\r\n    predict = y_prob > threshold\r\n    # print(\"measure测试：\", type(predict), predict.shape, len(predict))\r\n    if supervise == \"ranking loss\":\r\n        temp_ranking_loss = compute_ranking_loss_vec(y_prob, label)  # prob / y_prob\r\n        value = temp_ranking_loss\r\n    elif supervise == \"hamming loss\":\r\n        temp_hamming_loss = compute_hamming_loss_vec(predict, label)\r\n        value = temp_hamming_loss\r\n    elif supervise == \"one-error\":\r\n        temp_one_error = compute_one_error_vec(y_prob, label)\r\n        value = temp_one_error\r\n    elif supervise == \"average precision\":\r\n        temp_average_precision = compute_average_precision_vec(y_prob, label)\r\n        value = temp_average_precision\r\n    elif supervise == \"coverage\":\r\n        temp_coverage = compute_coverage_vec(y_prob, label)\r\n        value = temp_coverage\r\n    elif supervise == \"macro_auc\":\r\n        macro_auc = compute_auc_vec(y_prob, label)\r\n        value = macro_auc\r\n    return value\r\n\r\n\r\ndef update_supervise(supervise, value_pool, layer_index, y_prob, label, threshold):\r\n    back = False\r\n    back2 = False\r\n    value_pool[layer_index] = compute_supervise(\r\n        supervise, y_prob, label, threshold)\r\n    if layer_index >= 2 and compare_supervise_value(supervise, value_pool[layer_index - 2],\r\n                                                    value_pool[layer_index - 1]):\r\n        back2 = True\r\n    if layer_index >= 1 and compare_supervise_value(supervise, value_pool[layer_index - 1], value_pool[layer_index]):\r\n        back = True\r\n    return [back, back2]\r\n\r\n\r\n# 计算准确率\r\ndef compute_accuracy(pred_label, label):\r\n    # 先取得实例数\r\n    num_samples = len(label)\r\n    # 计算准确率\r\n    acc = sum(label == pred_label) * 1.0 / num_samples\r\n    return acc\r\n\r\n\r\n# 计算单个标签的性能\r\ndef compute_performance_single_label(predict_score, label):\r\n    \"\"\"\r\n    :param predict_score: 预测得分\r\n    :param label: 对应标签\r\n    :return:\r\n    \"\"\"\r\n    predict_label = predict_score > 0.5\r\n    _, num_labels = label.shape\r\n    acc = np.empty(num_labels)\r\n    f1 = np.empty(num_labels)\r\n    auc = np.empty(num_labels)\r\n    for i in range(num_labels):\r\n        acc[i] = metrics.accuracy_score(\r\n            label[:, i].reshape(-1), predict_label[:, i].reshape(-1))\r\n        f1[i] = metrics.f1_score(\r\n            label[:, i].reshape(-1), predict_label[:, i].reshape(-1))\r\n        auc[i] = metrics.roc_auc_score(\r\n            label[:, i].reshape(-1), predict_score[:, i].reshape(-1))\r\n    return [acc, f1, auc]\r\n\r\n\r\n# 计算排名\r\ndef compute_rank(y_prob):\r\n    rank = np.zeros(y_prob.shape)\r\n    for i in range(len(y_prob)):\r\n        temp = y_prob[i, :].argsort()\r\n        ranks = np.empty_like(temp)\r\n        ranks[temp] = np.arange(len(y_prob[i, :]))\r\n        rank[i, :] = ranks\r\n    return y_prob.shape[1] - rank\r\n\r\n\r\n# example based measure\r\ndef compute_hamming_loss(pred_label, label):\r\n    # 先计算准确率\r\n    acc = compute_accuracy(pred_label, label)\r\n    return 1 - acc.mean()\r\n\r\n\r\ndef compute_hamming_loss_vec(pred_label, label):\r\n    acc = compute_accuracy(pred_label, label)\r\n    return 1 - acc\r\n\r\n\r\n# label based measure\r\ndef compute_macro_f1(pred_label, label):\r\n    up = np.sum(pred_label * label, axis=0)\r\n    down = np.sum(pred_label, axis=0) + np.sum(label, axis=0)\r\n    if np.sum(np.sum(label, axis=0) == 0) > 0:\r\n        up[down == 0] = 0\r\n        down[down == 0] = 1\r\n    macro_f1 = 2.0 * np.sum(up / down)\r\n    macro_f1 = macro_f1 * 1.0 / label.shape[1]\r\n    return macro_f1\r\n\r\n\r\ndef compute_micro_f1(pred_label, label):\r\n    up = np.sum(pred_label * label)\r\n    down = np.sum(pred_label) + np.sum(label)\r\n    if np.sum(np.sum(label) == 0) > 0:\r\n        up[down == 0] = 0\r\n        down[down == 0] = 1\r\n    micro_f1 = 2.0 * up / down\r\n    return micro_f1\r\n\r\n\r\n# ranking based measure\r\ndef compute_ranking_loss(y_prob, label):\r\n    # y_predict = y_prob > 0.5\r\n    num_samples, num_labels = label.shape\r\n    loss = 0\r\n    for i in range(num_samples):\r\n        prob_positive = y_prob[i, label[i, :] > 0.5]\r\n        prob_negative = y_prob[i, label[i, :] < 0.5]\r\n        s = 0\r\n        for j in range(prob_positive.shape[0]):\r\n            for k in range(prob_negative.shape[0]):\r\n                if prob_negative[k] >= prob_positive[j]:\r\n                    s += 1\r\n\r\n        label_positive = np.sum(label[i, :] > 0.5)\r\n        label_negative = np.sum(label[i, :] < 0.5)\r\n        if label_negative != 0 and label_positive != 0:\r\n            loss = loss + s * 1.0 / (label_negative * label_positive)\r\n\r\n    return loss * 1.0 / num_samples\r\n\r\n\r\n# 计算排名损失\r\ndef compute_ranking_loss_vec(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    # 初始化损失矩阵，规模与实例数相同\r\n    loss = np.zeros(num_samples)\r\n    print(\"初始化排名损失矩阵,shape是：\", loss.shape)\r\n    for i in range(num_samples):\r\n        prob_positive = y_prob[i, label[i, :] > 0.5]\r\n        prob_negative = y_prob[i, label[i, :] < 0.5]\r\n        s = 0\r\n        for j in range(prob_positive.shape[0]):\r\n            for k in range(prob_negative.shape[0]):\r\n                if prob_negative[k] >= prob_positive[j]:\r\n                    s += 1\r\n\r\n        label_positive = np.sum(label[i, :] > 0.5)\r\n        label_negative = np.sum(label[i, :] < 0.5)\r\n        if label_negative != 0 and label_positive != 0:\r\n            loss[i] = s * 1.0 / (label_negative * label_positive)\r\n    return loss\r\n\r\n\r\ndef compute_one_error(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    loss = 0\r\n    for i in range(num_samples):\r\n        pos = np.argmax(y_prob[i, :])\r\n        loss += label[i, pos] < 0.5\r\n    return loss * 1.0 / num_samples\r\n\r\n\r\ndef compute_one_error_vec(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    loss = np.zeros(num_samples)\r\n    for i in range(num_samples):\r\n        pos = np.argmax(y_prob[i, :])\r\n        loss[i] = label[i, pos] < 0.5\r\n    return loss\r\n\r\n\r\ndef compute_coverage(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    rank = compute_rank(y_prob)\r\n    coverage = 0\r\n    for i in range(num_samples):\r\n        if sum(label[i, :] > 0.5) > 0:\r\n            coverage += max(rank[i, label[i, :] > 0.5])\r\n    coverage = coverage * 1.0 / num_samples - 1\r\n    return coverage / num_labels\r\n\r\n\r\ndef compute_coverage_vec(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    rank = compute_rank(y_prob)\r\n    coverage = np.zeros(num_samples)\r\n    for i in range(num_samples):\r\n        if sum(label[i, :] > 0.5) > 0:\r\n            coverage[i] = max(rank[i, label[i, :] > 0.5])\r\n    return coverage\r\n\r\n\r\ndef compute_average_precision(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    rank = compute_rank(y_prob)\r\n    precision = 0\r\n    for i in range(num_samples):\r\n        positive = np.sum(label[i, :] > 0.5)\r\n        rank_i = rank[i, label[i, :] > 0.5]\r\n        temp = rank_i.argsort()\r\n        ranks = np.empty_like(temp)\r\n        ranks[temp] = np.arange(len(rank_i))\r\n        ranks = ranks + 1\r\n        ans = ranks * 1.0 / rank_i\r\n        if positive > 0:\r\n            precision += np.sum(ans) * 1.0 / positive\r\n    return precision / num_samples\r\n\r\n\r\ndef compute_average_precision_vec(y_prob, label):\r\n    num_samples, num_labels = label.shape\r\n    rank = compute_rank(y_prob)\r\n    precision = np.zeros(num_samples)\r\n    for i in range(num_samples):\r\n        positive = np.sum(label[i, :] > 0.5)\r\n        rank_i = rank[i, label[i, :] > 0.5]\r\n        temp = rank_i.argsort()\r\n        ranks = np.empty_like(temp)\r\n        ranks[temp] = np.arange(len(rank_i))\r\n        ranks = ranks + 1\r\n        ans = ranks * 1.0 / rank_i\r\n        if positive > 0:\r\n            precision[i] = np.sum(ans) * 1.0 / positive\r\n    return precision\r\n\r\n\r\ndef compute_auc(y_prob, label):\r\n    n, m = label.shape\r\n    macro_auc = 0\r\n    valid_labels = 0\r\n    for i in range(m):\r\n        if np.unique(label[:, i]).shape[0] == 2:\r\n            index = np.argsort(y_prob[:, i])\r\n            pred = y_prob[:, i][index]\r\n            y = label[:, i][index] + 1\r\n            fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\r\n            temp = metrics.auc(fpr, tpr)\r\n            macro_auc += temp\r\n            valid_labels += 1\r\n    macro_auc /= valid_labels\r\n    return macro_auc\r\n\r\n\r\ndef compute_auc_vec(y_prob, label):\r\n    n, m = label.shape\r\n    macro_auc = np.zeros(m)\r\n    valid_labels = 0\r\n    for i in range(m):\r\n        if np.unique(label[:, i]).shape[0] == 2:\r\n            index = np.argsort(y_prob[:, i])\r\n            pred = y_prob[:, i][index]\r\n            y = label[:, i][index] + 1\r\n            fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\r\n            temp = metrics.auc(fpr, tpr)\r\n            macro_auc[i] = temp\r\n            valid_labels += 1\r\n    return macro_auc\r\n\r\n\r\n# metric(度量)是用于判断模型性能的函数。\r\ndef do_metric(y_prob, label, threshold):\r\n    # 和阈值比较\r\n    y_predict = y_prob > threshold\r\n    ranking_loss = compute_ranking_loss(y_prob, label)\r\n    # print(ranking_loss)\r\n    one_error = compute_one_error(y_prob, label)\r\n    # print(one_error)\r\n    coverage = compute_coverage(y_prob, label)\r\n    # print(coverage)\r\n    hamming_loss = compute_hamming_loss(y_predict, label)\r\n    # print(hamming_loss)\r\n    precision = compute_average_precision(y_prob, label)\r\n    # print(precision)\r\n    # macro_f1 = compute_macro_f1(y_predict, label)\r\n    # print(macro_f1)\r\n    # micro_f1 = compute_micro_f1(y_predict, label)\r\n    # print(micro_f1)\r\n    auc = compute_auc(y_prob, label)\r\n    return np.array([hamming_loss, one_error, coverage, ranking_loss, precision, auc])\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- learner/measure.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ learner/measure.py	(date 1607341552002)
@@ -1,5 +1,5 @@
 """
-六个度量指标
+指标类：六个度量指标，均是多标签学习的评估指标。
 """
 import numpy as np
 from sklearn import metrics
@@ -15,18 +15,19 @@
     elif supervise == "hamming loss":
         hamming_loss = 1.0
         return hamming_loss
+    # 一次误差
     elif supervise == "one-error":
         one_error = 1.0
         return one_error
+    # 平均精度
     elif supervise == "average precision":
         average_precision = 0.0
         return average_precision
+    # F1
     elif supervise == "micro-f1":
         micro_f1 = 0.0
         return micro_f1
-    elif supervise == "macro-f1":
-        macro_f1 = 0.0
-        return macro_f1
+    # 覆盖范围，覆盖率
     elif supervise == "coverage":
         coverage = 1000.0
         return coverage
@@ -350,18 +351,10 @@
     # 和阈值比较
     y_predict = y_prob > threshold
     ranking_loss = compute_ranking_loss(y_prob, label)
-    # print(ranking_loss)
     one_error = compute_one_error(y_prob, label)
-    # print(one_error)
     coverage = compute_coverage(y_prob, label)
-    # print(coverage)
     hamming_loss = compute_hamming_loss(y_predict, label)
-    # print(hamming_loss)
     precision = compute_average_precision(y_prob, label)
-    # print(precision)
-    # macro_f1 = compute_macro_f1(y_predict, label)
-    # print(macro_f1)
-    # micro_f1 = compute_micro_f1(y_predict, label)
-    # print(micro_f1)
+    macro_f1 = compute_macro_f1(y_predict, label)
     auc = compute_auc(y_prob, label)
-    return np.array([hamming_loss, one_error, coverage, ranking_loss, precision, auc])
+    return np.array([hamming_loss, one_error, coverage, ranking_loss, precision, macro_f1, auc])
Index: learner/cascade.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\n树级联成森林\r\n森林最大层数：20\r\n森林数：2\r\n\"\"\"\r\nfrom sklearn.cross_validation import KFold, StratifiedKFold\r\n# from sklearn.model_selection import KFold\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom .measure import *\r\nfrom .warpper import KfoldWarpper\r\n\r\n\r\nclass Cascade:\r\n    # 本实验将最大层数（T）设置为20\r\n    def __init__(self, dataname, max_layer=20, num_forests=2, n_fold=5, step=3):\r\n        \"\"\"\r\n        :param dataname: 数据集名称\r\n        :param max_layer: 森林最大层数，设为20\r\n        :param num_forests: 每一层的森林数量，设为2\r\n        :param n_fold: 每一层交叉验证倍数，设为5\r\n        :param step: 迭代次数，设为3\r\n        \"\"\"\r\n        self.max_layer = max_layer\r\n        self.n_fold = n_fold\r\n        self.step = step\r\n        self.layer_list = []\r\n        self.num_forests = num_forests\r\n        self.dataname = dataname\r\n        self.eta = []\r\n        self.model = []\r\n\r\n    # 针对六个多标签指标（用supervise表示），计算置信度，公式如论文中的表2所示，结果用alpha表示\r\n    # P是预测值矩阵\r\n    def compute_confidence(self, supervise, P):\r\n        \"\"\"\r\n        :param supervise: string (e.g. \"hamming loss\", \"one-error\")，即指标\r\n        :param P: array, whose shape is (num_samples, num_labels)\r\n        :return alpha: array, whose shape is :\r\n                        (num_samples, ) when supervise is instance-based measure,\r\n                        and (num_labels, ) when supervise is label-based measure\r\n        \"\"\"\r\n        m, l = P.shape[0], P.shape[1]\r\n        print(\"计算置信度，当前层的实例数、标签数：\", m, l)\r\n        if supervise == \"hamming loss\":\r\n            alpha = np.sum(np.abs(P - 0.5) + 0.5, axis=0) / m\r\n        elif supervise == \"one-error\":\r\n            alpha = np.max(P, axis=1)\r\n        elif supervise == \"ranking loss\" or supervise == \"average precision\":\r\n            forward_prod = np.sort(P, axis=1)\r\n            backward_prod = 1 - forward_prod\r\n            for j in range(1, l, 1):\r\n                forward_prod[:, j] = forward_prod[:, j - 1] * P[:, j]\r\n            for j in range(l - 2, -1, -1):\r\n                backward_prod[:, j] = backward_prod[:, j + 1] * (1 - P[:, j])\r\n            alpha = forward_prod[:, l - 1] + backward_prod[:, 0]\r\n            for j in range(l - 1):\r\n                alpha += forward_prod[:, j] * backward_prod[:, j + 1]\r\n        elif supervise == \"coverage\":\r\n            backward_prod = 1 - np.sort(P, axis=1)\r\n            for j in range(l - 2, -1, -1):\r\n                backward_prod[:, j] = backward_prod[:, j + 1] * (1 - P[:, j])\r\n            alpha = backward_prod[:, 0]\r\n            for j in range(l - 1):\r\n                alpha += j * P[:, j] * backward_prod[:, j + 1]\r\n            alpha = 1 - alpha / l\r\n        elif supervise == \"macro_auc\":\r\n            forward_prod = np.sort(P, axis=0)\r\n            backward_prod = 1 - P.copy()\r\n            for i in range(1, m, 1):\r\n                forward_prod[i, :] = forward_prod[i - 1, :] * P[i, :]\r\n            for i in range(m - 2, -1, -1):\r\n                backward_prod[i, :] = backward_prod[i + 1, :] * (1 - P[i, :])\r\n            alpha = forward_prod[m - 1, :] + backward_prod[0, :]\r\n            for i in range(m - 1):\r\n                alpha += forward_prod[i, :] * backward_prod[i + 1, :]\r\n        return alpha\r\n\r\n    # 在第一层中，每个森林中有40棵树，然后比上一层增加20棵树，直到树数达到100\r\n    # 形参中指定了参数默认值，但是调用时以实参为准\r\n    def train(self, train_data_raw, train_label_raw, supervise, n_estimators=40):\r\n        \"\"\"\r\n        :param train_data_raw: array, whose shape is (num_samples, num_features)\r\n        :param train_label_raw: array, whose shape is (num_samples, num_labels)\r\n        :param supervise: string, (e.g. \"hamming loss\", \"one-error\")\r\n        :param n_estimators: int, 每个森林块中树的数量，本实验中设为40\r\n        \"\"\"\r\n        # 将参数中的训练集、对应的标签集复制一份\r\n        train_data = train_data_raw.copy()\r\n        train_label = train_label_raw.copy()\r\n        # 标签数取的是训练标签集的列数\r\n        self.num_labels = train_label.shape[1]\r\n        # 初始化指标值，不同的指标初值不同\r\n        best_value = init_supervise(supervise)\r\n        bad = 0\r\n        # 初始化一个和train_label矩阵一样规模的矩阵，但元素不是空\r\n        best_train_prob = np.empty(train_label.shape)\r\n        # 初始化一个三维矩阵：每层的森林数、实例数、标签数\r\n        best_concatenate_prob = np.empty([self.num_forests, train_data.shape[0], self.num_labels])\r\n\r\n        print(\"$\" * 50)\r\n\r\n        # max_layer = 20，遍历森林的每一层，逐层训练\r\n        for layer_index in range(self.max_layer):\r\n            print(\"训练第\" + str(layer_index) + \"层ing\")\r\n\r\n            # K折交叉验证：用sklearn.cross_validation 求kf，此包已经弃用，但有n_folds参数\r\n            # 将训练数据集划分len(train_label)个互斥子集，\r\n            #       每次用其中一个子集当作验证集，剩下的len(train_label)-1个作为训练集，\r\n            #               进行len(train_label)次训练和测试，得到len(train_label)个结果\r\n            # 为了防止过拟合，我们对森林的每一层都做了K折交叉验证\r\n            # n_splits 表示划分为几块（至少是2）\r\n            # shuffle 表示是否打乱划分，默认False，即不打乱\r\n            # random_state 随机种子数,表示是否固定随机起点，Used when shuffle == True.\r\n            kf = KFold(len(train_label), n_folds=self.n_fold, shuffle=True, random_state=0)\r\n\r\n            # print(\"cross_validation求得kf:\", type(kf), kf)\r\n\r\n            # 用from sklearn.model_selection 求kf\r\n            # shuffle：在每次划分时，是否打乱\r\n            #     ①若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同\r\n            #     ②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的\r\n            # kf = KFold(len(train_label), shuffle=True, random_state=0).split(train_data.shape[0])\r\n\r\n            # 参数：森林数=2，每个森里中的树的数量=40，n_fold折交叉验证，层序号（1~20，for循环ing），步数=3\r\n            kfoldwarpper = KfoldWarpper(self.num_forests, n_estimators, self.n_fold, kf, layer_index, self.step)\r\n            # 参数：训练集、对应标签集；返回值是[预测值针对森林数取得均值， 按分类器存放的预测值]\r\n            prob, prob_concatenate = kfoldwarpper.train(train_data, train_label)\r\n\r\n            self.model.append(kfoldwarpper)\r\n            # 第一层\r\n            if layer_index == 0:\r\n                best_train_prob = prob\r\n                # 指标名称，训练标签集，阈值初值为0.5\r\n                pre_metric = compute_supervise_vec(supervise, best_train_prob, train_label, 0.5)\r\n            # 非第一层\r\n            else:\r\n                now_metric = compute_supervise_vec(supervise, prob, train_label, 0.5)\r\n                if supervise == \"average precision\" or supervise == \"macro_auc\":\r\n                    indicator = now_metric < pre_metric\r\n                else:\r\n                    indicator = now_metric > pre_metric\r\n\r\n                if np.sum(indicator) > 0:\r\n                    # 计算置信度\r\n                    confidence = self.compute_confidence(supervise, prob)\r\n                    # 取置信度均值作为阈值\r\n                    eta_t = np.mean(confidence[indicator])\r\n\r\n                    train_indicator = confidence < eta_t\r\n                    if supervise == \"hamming loss\" or supervise == \"macro_auc\":\r\n                        prob[:, train_indicator] = best_train_prob[:, train_indicator]\r\n                        prob_concatenate[:, :, train_indicator] = best_concatenate_prob[:, :, train_indicator]\r\n                    else:\r\n                        prob[train_indicator, :] = best_train_prob[train_indicator, :]\r\n                        prob_concatenate[:, train_indicator, :] = best_concatenate_prob[:, train_indicator, :]\r\n                else:\r\n                    eta_t = 0\r\n\r\n                self.eta.append(eta_t)\r\n\r\n                best_train_prob = prob\r\n\r\n                best_concatenate_prob = prob_concatenate\r\n\r\n                pre_metric = compute_supervise_vec(supervise, best_train_prob, train_label, 0.5)\r\n\r\n            value = compute_supervise(supervise, best_train_prob, train_label, 0.5)\r\n            back = compare_supervise_value(supervise, best_value, value)\r\n            if back:\r\n                bad += 1\r\n            else:\r\n                bad = 0\r\n                best_value = value\r\n            print(\"cascade测试bad：\", bad)\r\n            # 若近3层没有更新，则舍弃当前层模型和阈值\r\n            if bad >= 3:\r\n                for i in range(bad):\r\n                    self.model.pop()\r\n                    self.eta.pop()\r\n                break\r\n            # 准备下一层数据\r\n            # transpose函数：重新指定0，1，2三个轴的顺序\r\n            prob_concatenate = best_concatenate_prob.transpose((1, 0, 2))\r\n            prob_concatenate = prob_concatenate.reshape(prob_concatenate.shape[0], -1)\r\n            # 将prob_concatenate拼接到train_data_raw下面，行数会改变，所以axis=1\r\n            train_data = np.concatenate([train_data_raw.copy(), prob_concatenate], axis=1)\r\n\r\n    # 针对不同指标，对原始测试数据做预测\r\n    def predict(self, test_data_raw, supervise):\r\n        \"\"\"\r\n        :param test_data_raw: array, whose shape is (num_test_samples, num_features)\r\n        :return prob: array, whose shape is (num_test_samples, num_labels)\r\n        \"\"\"\r\n        test_data = test_data_raw.copy()\r\n        best_prob = np.empty([test_data.shape[0], self.num_labels])\r\n        best_concatenate_prob = np.empty([self.num_forests, test_data.shape[0], self.num_labels])\r\n        # zip()函数，两参数中的两迭代对象一一对应并打包成新对象的一个元素\r\n        # 遍历每层的分类器和每层的阈值\r\n        for clf, eta_t in zip(self.model, self.eta):\r\n            # 分类器预测test_data，得到[预测值针对森林数取的均值， 按分类器存放的预测值]\r\n            prob, prob_concatenate = clf.predict(test_data)\r\n            confidence = self.compute_confidence(supervise, prob)\r\n            indicator = confidence < eta_t\r\n            # print(indicator)\r\n            if supervise == \"hamming loss\" or supervise == \"macro_auc\":\r\n                prob[:, indicator] = best_prob[:, indicator]\r\n                prob_concatenate[:, :, indicator] = best_concatenate_prob[:, :, indicator]\r\n            else:\r\n                prob[indicator, :] = best_prob[indicator, :]\r\n                prob_concatenate[:, indicator, :] = best_concatenate_prob[:, indicator, :]\r\n            best_concatenate_prob = prob_concatenate\r\n            best_prob = prob\r\n            prob_concatenate = best_concatenate_prob.transpose((1, 0, 2))\r\n            prob_concatenate = prob_concatenate.reshape(prob_concatenate.shape[0], -1)\r\n            test_data = np.concatenate([test_data_raw.copy(), prob_concatenate], axis=1)\r\n        return best_prob\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- learner/cascade.py	(revision 8018a5ae37ff9389bef48209b13967e2938b39dd)
+++ learner/cascade.py	(date 1607350101302)
@@ -12,14 +12,15 @@
 
 
 class Cascade:
-    # 本实验将最大层数（T）设置为20
-    def __init__(self, dataname, max_layer=20, num_forests=2, n_fold=5, step=3):
+    # 本实验将最大层数（T）设置为20；python函数调用，参数列表以实参为准
+    def __init__(self, dataname, max_layer=20, num_forests=4, n_fold=5, step=3):
         """
         :param dataname: 数据集名称
         :param max_layer: 森林最大层数，设为20
-        :param num_forests: 每一层的森林数量，设为2
+        :param num_forests: 每一层的森林数量，设为4
         :param n_fold: 每一层交叉验证倍数，设为5
-        :param step: 迭代次数，设为3
+        :param step: 第一层的最大深度设置为3，然后在增加层数时比上一层多3倍
+        :param eta: 存放各层的阈值
         """
         self.max_layer = max_layer
         self.n_fold = n_fold
@@ -92,17 +93,23 @@
         self.num_labels = train_label.shape[1]
         # 初始化指标值，不同的指标初值不同
         best_value = init_supervise(supervise)
+        # bad用来累计没有改进的层数，近三层没有明显改进，则停止训练
         bad = 0
         # 初始化一个和train_label矩阵一样规模的矩阵，但元素不是空
         best_train_prob = np.empty(train_label.shape)
         # 初始化一个三维矩阵：每层的森林数、实例数、标签数
         best_concatenate_prob = np.empty([self.num_forests, train_data.shape[0], self.num_labels])
 
-        print("$" * 50)
+        print("级联结构开始训练："
+              "\n评估指标是：", supervise,
+              "\n模型最大层数：", self.max_layer,
+              "\n每一层级联森林数：", self.num_forests,
+              "\n交叉验证倍数：", self.n_fold,
+              "\n没个森林中树的数量：", n_estimators)
 
         # max_layer = 20，遍历森林的每一层，逐层训练
         for layer_index in range(self.max_layer):
-            print("训练第" + str(layer_index) + "层ing")
+            print("训练MLDF模型的第" + str(layer_index) + "层ing")
 
             # K折交叉验证：用sklearn.cross_validation 求kf，此包已经弃用，但有n_folds参数
             # 将训练数据集划分len(train_label)个互斥子集，
@@ -112,9 +119,10 @@
             # n_splits 表示划分为几块（至少是2）
             # shuffle 表示是否打乱划分，默认False，即不打乱
             # random_state 随机种子数,表示是否固定随机起点，Used when shuffle == True.
+            # 返回值是：train_index，test_index——训练集的索引，验证集的索引
             kf = KFold(len(train_label), n_folds=self.n_fold, shuffle=True, random_state=0)
 
-            # print("cross_validation求得kf:", type(kf), kf)
+            # print("cross_validation求得kf:", type(kf), len(kf)) # <class 'sklearn.cross_validation.KFold'>  5
 
             # 用from sklearn.model_selection 求kf
             # shuffle：在每次划分时，是否打乱
@@ -122,7 +130,7 @@
             #     ②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的
             # kf = KFold(len(train_label), shuffle=True, random_state=0).split(train_data.shape[0])
 
-            # 参数：森林数=2，每个森里中的树的数量=40，n_fold折交叉验证，层序号（1~20，for循环ing），步数=3
+            # 参数：森林数=2，每个森里中的树的数量=40，5折交叉验证，层序号（1~20，for循环ing），步数=3
             kfoldwarpper = KfoldWarpper(self.num_forests, n_estimators, self.n_fold, kf, layer_index, self.step)
             # 参数：训练集、对应标签集；返回值是[预测值针对森林数取得均值， 按分类器存放的预测值]
             prob, prob_concatenate = kfoldwarpper.train(train_data, train_label)
@@ -137,6 +145,7 @@
             else:
                 now_metric = compute_supervise_vec(supervise, prob, train_label, 0.5)
                 if supervise == "average precision" or supervise == "macro_auc":
+                    # indicator:指示器
                     indicator = now_metric < pre_metric
                 else:
                     indicator = now_metric > pre_metric
@@ -172,7 +181,6 @@
             else:
                 bad = 0
                 best_value = value
-            print("cascade测试bad：", bad)
             # 若近3层没有更新，则舍弃当前层模型和阈值
             if bad >= 3:
                 for i in range(bad):
@@ -201,8 +209,9 @@
             # 分类器预测test_data，得到[预测值针对森林数取的均值， 按分类器存放的预测值]
             prob, prob_concatenate = clf.predict(test_data)
             confidence = self.compute_confidence(supervise, prob)
+            print("概率矩阵：", prob.shape, prob)
             indicator = confidence < eta_t
-            # print(indicator)
+            print("cascade类中打印indicator:", indicator)
             if supervise == "hamming loss" or supervise == "macro_auc":
                 prob[:, indicator] = best_prob[:, indicator]
                 prob_concatenate[:, :, indicator] = best_concatenate_prob[:, :, indicator]
diff --git Iris/__init__.py Iris/__init__.py
deleted file mode 100644
